{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function \nfrom __future__ import division\n\n%matplotlib inline\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch.utils.data import Dataset, random_split, DataLoader\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nfrom PIL import Image\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = 1337\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_s\n\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:36.80292Z","iopub.execute_input":"2022-04-15T09:15:36.80312Z","iopub.status.idle":"2022-04-15T09:15:36.810501Z","shell.execute_reply.started":"2022-04-15T09:15:36.803094Z","shell.execute_reply":"2022-04-15T09:15:36.809757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device('cpu')\nif torch.cuda.is_available():\n    DEVICE = torch.device('cuda:0')\n    torch.cuda.manual_seed(RANDOM_SEED)\nDEVICE","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:36.811624Z","iopub.execute_input":"2022-04-15T09:15:36.811908Z","iopub.status.idle":"2022-04-15T09:15:36.87499Z","shell.execute_reply.started":"2022-04-15T09:15:36.811868Z","shell.execute_reply":"2022-04-15T09:15:36.874215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set data path\nDATA_DIR = '../input/dog-breed-identification'\nTRAIN_DIR = DATA_DIR + '/train'                           \nTEST_DIR = DATA_DIR + '/test'                             \n\nTRAIN_CSV = DATA_DIR + '/labels.csv'                     \nTEST_CSV = DATA_DIR + '/submission.csv' \n\nmodel_name = \"resnet\"\nnum_classes = 120\nbatch_size = 16\nnum_epochs = 55\n\n# Flag for feature extracting. When False, we finetune the whole model, \n#   when True we only update the reshaped layer params\nfeature_extract = True","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:36.876137Z","iopub.execute_input":"2022-04-15T09:15:36.876403Z","iopub.status.idle":"2022-04-15T09:15:36.883486Z","shell.execute_reply.started":"2022-04-15T09:15:36.876367Z","shell.execute_reply":"2022-04-15T09:15:36.882488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv(TRAIN_CSV)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:36.886602Z","iopub.execute_input":"2022-04-15T09:15:36.887203Z","iopub.status.idle":"2022-04-15T09:15:36.93046Z","shell.execute_reply.started":"2022-04-15T09:15:36.88716Z","shell.execute_reply":"2022-04-15T09:15:36.92978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_names=df[\"breed\"].unique()\nlabels_sorted=labels_names.sort()\nlabels = dict(zip(range(len(labels_names)),labels_names))\nlabels ","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:36.931735Z","iopub.execute_input":"2022-04-15T09:15:36.932181Z","iopub.status.idle":"2022-04-15T09:15:36.947448Z","shell.execute_reply.started":"2022-04-15T09:15:36.932146Z","shell.execute_reply":"2022-04-15T09:15:36.946797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label=[]\nfor i in range(len(df[\"breed\"])):\n    temp=list(labels.values()).index(df.breed[i])\n    label.append(temp)\n\n    \ndf['label'] = label\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:36.948665Z","iopub.execute_input":"2022-04-15T09:15:36.949095Z","iopub.status.idle":"2022-04-15T09:15:37.09478Z","shell.execute_reply.started":"2022-04-15T09:15:36.949061Z","shell.execute_reply":"2022-04-15T09:15:37.094146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_img=[]\nfor i in range(len(df[\"id\"])):\n    temp=TRAIN_DIR + \"/\" + str(df.id[i]) + \".jpg\"\n    path_img.append(temp)\n\ndf['path_img'] =path_img\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:37.096033Z","iopub.execute_input":"2022-04-15T09:15:37.096267Z","iopub.status.idle":"2022-04-15T09:15:37.220934Z","shell.execute_reply.started":"2022-04-15T09:15:37.096235Z","shell.execute_reply":"2022-04-15T09:15:37.220275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DogDataset(Dataset):\n    def __init__(self, df, root_dir, transform=None):\n        self.df = df\n        self.transform = transform\n        self.root_dir = root_dir\n        \n    def __len__(self):\n        return len(self.df)    \n    \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        img_id, img_label = row['id'], row['label']\n        img_fname = self.root_dir + \"/\" + str(img_id) + \".jpg\"\n        img = Image.open(img_fname)\n        if self.transform:\n            img = self.transform(img)\n        return img, img_label","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:37.222156Z","iopub.execute_input":"2022-04-15T09:15:37.222403Z","iopub.status.idle":"2022-04-15T09:15:37.229083Z","shell.execute_reply.started":"2022-04-15T09:15:37.22237Z","shell.execute_reply":"2022-04-15T09:15:37.22828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:37.231226Z","iopub.execute_input":"2022-04-15T09:15:37.231739Z","iopub.status.idle":"2022-04-15T09:15:37.237563Z","shell.execute_reply.started":"2022-04-15T09:15:37.231592Z","shell.execute_reply":"2022-04-15T09:15:37.236825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Model","metadata":{}},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'test']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(DEVICE)\n                labels = labels.to(DEVICE)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Get model outputs and calculate loss\n                    # Special case for inception because in training it has an auxiliary output. In train\n                    #   mode we calculate the loss by summing the final output and the auxiliary output\n                    #   but in testing we only consider the final output.\n                    if is_inception and phase == 'train':\n                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'test' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'test':\n                val_acc_history.append(epoch_acc)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history\n    return self.blocks(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:37.239837Z","iopub.execute_input":"2022-04-15T09:15:37.24039Z","iopub.status.idle":"2022-04-15T09:15:37.256025Z","shell.execute_reply.started":"2022-04-15T09:15:37.240354Z","shell.execute_reply":"2022-04-15T09:15:37.255056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"alexnet\":\n        \"\"\" Alexnet\n        \"\"\"\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn\n        \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n        \"\"\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n        input_size = 224\n\n    elif model_name == \"inception\":\n        \"\"\" Inception v3 \n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n    \n    return model_ft, input_size\n\n# Initialize the model for this run\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n\n# Print the model we just instantiated\nprint(model_ft)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:37.257581Z","iopub.execute_input":"2022-04-15T09:15:37.258269Z","iopub.status.idle":"2022-04-15T09:15:43.133387Z","shell.execute_reply.started":"2022-04-15T09:15:37.25823Z","shell.execute_reply":"2022-04-15T09:15:43.132622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(45),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((224, 224)),\n#         transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\nprint(\"Initializing Datasets and Dataloaders...\")\n\nnp.random.seed(42)\nmsk = np.random.rand(len(df)) < 0.8\n\ntrain_df = df[msk].reset_index()\nval_df = df[~msk].reset_index()\n\ntrain_data = DogDataset(train_df, TRAIN_DIR, transform=data_transforms['train'])\nval_data = DogDataset(val_df, TRAIN_DIR, transform=data_transforms['test'])\n\n\n# # Create training and validation datasets\nimage_datasets = {'train':train_data,'test':val_data}\n# Create training and validation dataloaders\ndataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in ['train', 'test']}\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:43.134721Z","iopub.execute_input":"2022-04-15T09:15:43.135121Z","iopub.status.idle":"2022-04-15T09:15:43.152964Z","shell.execute_reply.started":"2022-04-15T09:15:43.135082Z","shell.execute_reply":"2022-04-15T09:15:43.152144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"# Send the model to GPU\nmodel_ft = model_ft.to(DEVICE)\n\n# Gather the parameters to be optimized/updated in this run. If we are\n#  finetuning we will be updating all parameters. However, if we are \n#  doing feature extract method, we will only update the parameters\n#  that we have just initialized, i.e. the parameters with requires_grad\n#  is True.\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:43.156113Z","iopub.execute_input":"2022-04-15T09:15:43.156536Z","iopub.status.idle":"2022-04-15T09:15:45.794759Z","shell.execute_reply.started":"2022-04-15T09:15:43.156503Z","shell.execute_reply":"2022-04-15T09:15:45.793975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup the loss fxn\ncriterion = nn.CrossEntropyLoss()\n\n# Train and evaluate\nmodel_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T09:15:45.79615Z","iopub.execute_input":"2022-04-15T09:15:45.796414Z","iopub.status.idle":"2022-04-15T10:06:17.811322Z","shell.execute_reply.started":"2022-04-15T09:15:45.796377Z","shell.execute_reply":"2022-04-15T10:06:17.809753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the non-pretrained version of the model used for this run\n# scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n# scratch_model = scratch_model.to(DEVICE)\n# scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=3e-4, momentum=0.9)\n# scratch_criterion = nn.CrossEntropyLoss()\n# _,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n\n# Plot the training curves of validation accuracy vs. number \n#  of training epochs for the transfer learning method and\n#  the model trained from scratch\nohist = []\n# shist = []\n\nohist = [h.cpu().numpy() for h in hist]\n# shist = [h.cpu().numpy() for h in scratch_hist]\n\nplt.title(\"Validation Accuracy vs. Number of Training Epochs\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Validation Accuracy\")\nplt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n# plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\nplt.ylim((0,1.))\nplt.xticks(np.arange(1, num_epochs+1, 1.0))\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:59:01.564204Z","iopub.execute_input":"2022-04-15T10:59:01.564557Z","iopub.status.idle":"2022-04-15T10:59:02.241728Z","shell.execute_reply.started":"2022-04-15T10:59:01.56452Z","shell.execute_reply":"2022-04-15T10:59:02.241027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Model","metadata":{}},{"cell_type":"code","source":"torch.save(model_ft.state_dict(), './dog_model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:59:02.243306Z","iopub.execute_input":"2022-04-15T10:59:02.24373Z","iopub.status.idle":"2022-04-15T10:59:02.334109Z","shell.execute_reply.started":"2022-04-15T10:59:02.243691Z","shell.execute_reply":"2022-04-15T10:59:02.333365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model_ft, './dog_model.pt')","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:59:02.335576Z","iopub.execute_input":"2022-04-15T10:59:02.335889Z","iopub.status.idle":"2022-04-15T10:59:02.415223Z","shell.execute_reply.started":"2022-04-15T10:59:02.335851Z","shell.execute_reply":"2022-04-15T10:59:02.414325Z"},"trusted":true},"execution_count":null,"outputs":[]}]}